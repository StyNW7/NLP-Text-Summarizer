{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10fe549d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from datasets import load_from_disk\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "from evaluate import load as load_metric\n",
    "\n",
    "def get_max_input_length(model_name):\n",
    "    if \"pegasus\" in model_name:\n",
    "        return 512\n",
    "    elif \"longformer\" in model_name:\n",
    "        return 4096\n",
    "    else:\n",
    "        return 1024\n",
    "\n",
    "def generate_summaries(model_name, dataset_split=\"test\"):\n",
    "    model_dir = f\"../models/{model_name.replace('/', '_')}\"\n",
    "    model = AutoModelForSeq2SeqLM.from_pretrained(model_dir).to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_dir)\n",
    "\n",
    "    dataset = load_from_disk(\"../data/cleaned_aclsum\")[dataset_split].select(range(10))\n",
    "\n",
    "    predictions = []\n",
    "    references = []\n",
    "    max_len = get_max_input_length(model_name)\n",
    "\n",
    "    print(f\"‚è≥ Generating summaries for {model_name}...\")\n",
    "    for entry in dataset:\n",
    "        if entry[\"document\"].strip() == \"\" or entry[\"outcome\"].strip() == \"\":\n",
    "            continue\n",
    "        inputs = tokenizer(entry[\"document\"], return_tensors=\"pt\", truncation=True, max_length=max_len).to(model.device)\n",
    "        summary_ids = model.generate(inputs.input_ids, max_length=150, num_beams=4)\n",
    "        pred = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "        predictions.append(pred)\n",
    "        references.append(entry[\"outcome\"])\n",
    "\n",
    "    return predictions, references\n",
    "\n",
    "def compute_metrics(predictions, references):\n",
    "    rouge = load_metric(\"rouge\")\n",
    "    result = rouge.compute(predictions=predictions, references=references, use_stemmer=True)\n",
    "    return {\n",
    "        \"ROUGE-1\": result[\"rouge1\"],\n",
    "        \"ROUGE-2\": result[\"rouge2\"],\n",
    "        \"ROUGE-L\": result[\"rougeL\"]\n",
    "    }\n",
    "\n",
    "def main():\n",
    "    models = [\n",
    "        \"t5-small\",\n",
    "        \"facebook/bart-base\",\n",
    "        \"google/pegasus-xsum\",\n",
    "        \"allenai/led-base-16384\"\n",
    "    ]\n",
    "    rows = []\n",
    "    for m in models:\n",
    "        preds, refs = generate_summaries(m)\n",
    "        metrics = compute_metrics(preds, refs)\n",
    "        metrics[\"Model\"] = m\n",
    "        rows.append(metrics)\n",
    "\n",
    "    df = pd.DataFrame(rows)\n",
    "    df = df[[\"Model\", \"ROUGE-1\", \"ROUGE-2\", \"ROUGE-L\"]]\n",
    "\n",
    "    # Save\n",
    "    os.makedirs(\"../results\", exist_ok=True)\n",
    "    df.to_csv(\"../results/model_comparison.csv\", index=False)\n",
    "\n",
    "    # Visualize\n",
    "    print(\"\\nüìä Results of Model Comparison:\")\n",
    "    print(df)\n",
    "\n",
    "    df.set_index(\"Model\").plot(kind=\"bar\", figsize=(10, 6))\n",
    "    plt.title(\"Model Summarization Comparison (ROUGE Scores)\")\n",
    "    plt.ylabel(\"Score\")\n",
    "    plt.ylim(0.0, 1.0)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"../results/model_comparison.png\")\n",
    "    plt.close()\n",
    "    print(\"‚úÖ Graph save at: results/model_comparison.png\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
