{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10fe549d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, load_from_disk\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "from evaluate import load as load_metric\n",
    "import pandas as pd\n",
    "\n",
    "def generate_summaries(model_name, dataset_split=\"test\"):\n",
    "\n",
    "    model = AutoModelForSeq2SeqLM.from_pretrained(f\"../models/{model_name.replace('/', '_')}\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(f\"../models/{model_name.replace('/', '_')}\")\n",
    "\n",
    "    # Original Dataset\n",
    "    # dataset = load_dataset(\"sobamchan/aclsum\")[dataset_split]\n",
    "\n",
    "    # Preprocess Dataset\n",
    "    dataset = load_from_disk(\"../data/cleaned_aclsum\")[dataset_split].select(range(10))\n",
    "    \n",
    "    predictions = []\n",
    "    references = []\n",
    "    \n",
    "    for entry in dataset:\n",
    "        input_ids = tokenizer(entry[\"document\"], return_tensors=\"pt\", truncation=True, max_length=1024).input_ids\n",
    "        summary_ids = model.generate(input_ids, max_length=150, num_beams=4)\n",
    "        pred = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "        predictions.append(pred)\n",
    "        references.append(entry[\"outcome\"])\n",
    "    \n",
    "    return predictions, references\n",
    "\n",
    "def compute_metrics(predictions, references):\n",
    "    rouge = load_metric(\"rouge\")\n",
    "    result = rouge.compute(predictions=predictions, references=references)\n",
    "    return {\n",
    "        \"ROUGE-1\": result[\"rouge1\"].mid.fmeasure,\n",
    "        \"ROUGE-2\": result[\"rouge2\"].mid.fmeasure,\n",
    "        \"ROUGE-L\": result[\"rougeL\"].mid.fmeasure\n",
    "    }\n",
    "\n",
    "def main():\n",
    "    models = [\"t5-small\", \"facebook/bart-base\", \"google/pegasus-xsum\", \"allenai/led-base-16384\"]\n",
    "    rows = []\n",
    "    for m in models:\n",
    "        print(f\"Evaluating {m}...\")\n",
    "        preds, refs = generate_summaries(m)\n",
    "        metrics = compute_metrics(preds, refs)\n",
    "        metrics[\"Model\"] = m\n",
    "        rows.append(metrics)\n",
    "    \n",
    "    df = pd.DataFrame(rows)\n",
    "    df.to_csv(\"../results/rouge_scores.csv\", index=False)\n",
    "    print(df)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
