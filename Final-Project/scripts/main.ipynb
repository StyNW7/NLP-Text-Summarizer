{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3f53bcfe",
   "metadata": {},
   "source": [
    "# <b>ACLSum Text Summarizer - Data Training, Modelling, and Evaluating</b>\n",
    "## 2702217125 - Stanley Nathanael Wijaya"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02de76cb",
   "metadata": {},
   "source": [
    "GitHub Repository Full Project: https://github.com/StyNW7/NLP-Text-Summarizer\n",
    "<br>\n",
    "Dataset Source: https://huggingface.co/datasets/sobamchan/aclsum\n",
    "<br>\n",
    "Docs Documentation: https://docs.google.com/document/d/1qSS2kVPMKn032hhjPmrgMquIb6Q827EiowY9y0s_k3I/edit?usp=sharing\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbc3172e",
   "metadata": {},
   "source": [
    "## Task Description\n",
    "\n",
    "In the Natural Language Processing Project at BINUS University, I want to compare Text Summarizer models using ACLSum Datasets.\n",
    "<br><br>\n",
    "Dataset:\n",
    "<br>\n",
    "https://huggingface.co/datasets/sobamchan/aclsum\n",
    "<br><br>\n",
    "To-do list in this Notebook:\n",
    "\n",
    "<br>\n",
    "Training and Evaluating\n",
    "<ul>\n",
    "    <li>Perform Training and record the model training result metrics</li>\n",
    "    <li>Compare the model you choose with at least 3 other models!</li>\n",
    "    <li>Conduct an evaluation using metrics according to the context of the topic</li>\n",
    "</ul>\n",
    "\n",
    "<br>\n",
    "Modelling\n",
    "<ul>\n",
    "    <li>Give reasons why you chose and used the model.</li>\n",
    "    <li>Provide an explanation of the architecture of the model you chose.</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6ae5530",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4ed6120",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import string\n",
    "import nltk\n",
    "from datasets import load_dataset, DatasetDict\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"wordnet\")\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    words = word_tokenize(text)\n",
    "    words = [lemmatizer.lemmatize(word) for word in words if word not in stop_words]\n",
    "    return \" \".join(words)\n",
    "\n",
    "def preprocess_dataset():\n",
    "    raw_dataset = load_dataset(\"sobamchan/aclsum\")\n",
    "    \n",
    "    def apply_cleaning(example):\n",
    "        example[\"document\"] = clean_text(example[\"document\"])\n",
    "        example[\"outcome\"] = clean_text(example[\"outcome\"])\n",
    "        return example\n",
    "    \n",
    "    cleaned_dataset = raw_dataset.map(apply_cleaning)\n",
    "    return cleaned_dataset\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    dataset = preprocess_dataset()\n",
    "    print(dataset)\n",
    "\n",
    "    output_path = \"../data/cleaned_aclsum\"\n",
    "    os.makedirs(output_path, exist_ok=True)\n",
    "    dataset.save_to_disk(output_path)\n",
    "\n",
    "    print(f\"Dataset yang telah dibersihkan berhasil disimpan di: {output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c46959c0",
   "metadata": {},
   "source": [
    "## Training Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d32ab56b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "from transformers import (\n",
    "    AutoTokenizer, AutoModelForSeq2SeqLM,\n",
    "    Seq2SeqTrainer, Seq2SeqTrainingArguments,\n",
    "    DataCollatorForSeq2Seq\n",
    ")\n",
    "from datasets import load_from_disk\n",
    "\n",
    "\n",
    "# Get Max Length Input for each model\n",
    "def get_max_input_length(model_checkpoint):\n",
    "    if \"pegasus\" in model_checkpoint:\n",
    "        return 512\n",
    "    elif \"longformer\" in model_checkpoint:\n",
    "        return 4096\n",
    "    else:\n",
    "        return 1024\n",
    "\n",
    "\n",
    "# Tokenization function with dynamic input length\n",
    "def tokenize_function(example, tokenizer, max_input_length=1024):\n",
    "    model_inputs = tokenizer(\n",
    "        example[\"document\"],\n",
    "        max_length=max_input_length,\n",
    "        padding=\"max_length\",\n",
    "        truncation=True\n",
    "    )\n",
    "    \n",
    "    labels = tokenizer(\n",
    "        text_target=example[\"outcome\"],\n",
    "        max_length=150,\n",
    "        padding=\"max_length\",\n",
    "        truncation=True\n",
    "    )\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "\n",
    "def main(model_checkpoint):\n",
    "    print(f\"üîß Loading model & tokenizer from: {model_checkpoint}\")\n",
    "    model = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoint)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "\n",
    "    print(\"üìÇ Loading preprocessed dataset...\")\n",
    "    dataset = load_from_disk(\"../data/cleaned_aclsum\")\n",
    "\n",
    "    max_input_length = get_max_input_length(model_checkpoint)\n",
    "    print(f\"üìè Using max_input_length: {max_input_length}\")\n",
    "\n",
    "    print(\"üî† Tokenizing dataset...\")\n",
    "    tokenized = dataset.map(\n",
    "        lambda x: tokenize_function(x, tokenizer, max_input_length=max_input_length),\n",
    "        batched=True\n",
    "    )\n",
    "\n",
    "    training_args = Seq2SeqTrainingArguments(\n",
    "        output_dir=f\"../models/{model_checkpoint.replace('/', '_')}\",\n",
    "        per_device_train_batch_size=4,\n",
    "        per_device_eval_batch_size=4,\n",
    "        predict_with_generate=True,\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        save_total_limit=2,\n",
    "        num_train_epochs=3,\n",
    "        logging_dir='./logs',\n",
    "        logging_steps=10,\n",
    "        learning_rate=2e-5,\n",
    "    )\n",
    "\n",
    "    trainer = Seq2SeqTrainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=tokenized[\"train\"],\n",
    "        eval_dataset=tokenized[\"validation\"],\n",
    "        tokenizer=tokenizer,\n",
    "        data_collator=DataCollatorForSeq2Seq(tokenizer, model=model)\n",
    "    )\n",
    "\n",
    "    print(\"üöÄ Training started...\")\n",
    "    trainer.train()\n",
    "\n",
    "    print(f\"üíæ Saving model to: ../models/{model_checkpoint.replace('/', '_')}\")\n",
    "    model.save_pretrained(f\"../models/{model_checkpoint.replace('/', '_')}\")\n",
    "    tokenizer.save_pretrained(f\"../models/{model_checkpoint.replace('/', '_')}\")\n",
    "\n",
    "    print(\"‚úÖ Training and saving completed.\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--model\", type=str, required=True)\n",
    "    args = parser.parse_args()\n",
    "    main(args.model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f444df5b",
   "metadata": {},
   "source": [
    "## Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83031d73",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from datasets import load_from_disk\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "from evaluate import load as load_metric\n",
    "\n",
    "def get_max_input_length(model_name):\n",
    "    if \"pegasus\" in model_name:\n",
    "        return 512\n",
    "    elif \"longformer\" in model_name:\n",
    "        return 4096\n",
    "    else:\n",
    "        return 1024\n",
    "\n",
    "def generate_summaries(model_name, dataset_split=\"test\"):\n",
    "    model_dir = f\"../models/{model_name.replace('/', '_')}\"\n",
    "    model = AutoModelForSeq2SeqLM.from_pretrained(model_dir).to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_dir)\n",
    "\n",
    "    dataset = load_from_disk(\"../data/cleaned_aclsum\")[dataset_split].select(range(10))\n",
    "\n",
    "    predictions = []\n",
    "    references = []\n",
    "    max_len = get_max_input_length(model_name)\n",
    "\n",
    "    print(f\"‚è≥ Generating summaries for {model_name}...\")\n",
    "    for entry in dataset:\n",
    "        if entry[\"document\"].strip() == \"\" or entry[\"outcome\"].strip() == \"\":\n",
    "            continue\n",
    "        inputs = tokenizer(entry[\"document\"], return_tensors=\"pt\", truncation=True, max_length=max_len).to(model.device)\n",
    "        summary_ids = model.generate(inputs.input_ids, max_length=150, num_beams=4)\n",
    "        pred = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "        predictions.append(pred)\n",
    "        references.append(entry[\"outcome\"])\n",
    "\n",
    "    return predictions, references\n",
    "\n",
    "def compute_metrics(predictions, references):\n",
    "    rouge = load_metric(\"rouge\")\n",
    "    result = rouge.compute(predictions=predictions, references=references, use_stemmer=True)\n",
    "    return {\n",
    "        \"ROUGE-1\": result[\"rouge1\"],\n",
    "        \"ROUGE-2\": result[\"rouge2\"],\n",
    "        \"ROUGE-L\": result[\"rougeL\"]\n",
    "    }\n",
    "\n",
    "def main():\n",
    "    models = [\n",
    "        \"t5-small\",\n",
    "        \"facebook/bart-base\",\n",
    "        \"google/pegasus-xsum\",\n",
    "        \"allenai/led-base-16384\"\n",
    "    ]\n",
    "    rows = []\n",
    "    for m in models:\n",
    "        preds, refs = generate_summaries(m)\n",
    "        metrics = compute_metrics(preds, refs)\n",
    "        metrics[\"Model\"] = m\n",
    "        rows.append(metrics)\n",
    "\n",
    "    df = pd.DataFrame(rows)\n",
    "    df = df[[\"Model\", \"ROUGE-1\", \"ROUGE-2\", \"ROUGE-L\"]]\n",
    "\n",
    "    # Save\n",
    "    os.makedirs(\"../results\", exist_ok=True)\n",
    "    df.to_csv(\"../results/model_comparison.csv\", index=False)\n",
    "\n",
    "    # Visualize\n",
    "    print(\"\\nüìä Results of Model Comparison:\")\n",
    "    print(df)\n",
    "\n",
    "    df.set_index(\"Model\").plot(kind=\"bar\", figsize=(10, 6))\n",
    "    plt.title(\"Model Summarization Comparison (ROUGE Scores)\")\n",
    "    plt.ylabel(\"Score\")\n",
    "    plt.ylim(0.0, 1.0)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"../results/model_comparison.png\")\n",
    "    plt.close()\n",
    "    print(\"‚úÖ Graph save at: results/model_comparison.png\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "496e71a7",
   "metadata": {},
   "source": [
    "## Visualize Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a7c9e17",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "data = {\n",
    "    \"Model\": [\"t5-small\", \"facebook/bart-base\", \"google/pegasus-xsum\", \"allenai/led-base-16384\"],\n",
    "    \"ROUGE-1\": [0.1369, 0.1236, 0.1018, 0.1643],\n",
    "    \"ROUGE-2\": [0.0359, 0.0340, 0.0463, 0.0685],\n",
    "    \"ROUGE-L\": [0.1013, 0.1163, 0.0922, 0.1522]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "df_plot = df.set_index(\"Model\")\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "df_plot.plot(kind=\"bar\", ax=plt.gca(), color=[\"#1f77b4\", \"#ff7f0e\", \"#2ca02c\"])\n",
    "plt.title(\"ROUGE Scores Comparison Across Models\", fontsize=14)\n",
    "plt.xlabel(\"Model\", fontsize=12)\n",
    "plt.ylabel(\"Score\", fontsize=12)\n",
    "plt.legend(title=\"Metric\")\n",
    "plt.grid(True, axis=\"y\", linestyle=\"--\", alpha=0.7)\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.savefig(\"../results/rouge_comparison.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9925732c",
   "metadata": {},
   "source": [
    "## Owner\n",
    "\n",
    "This Notebook is created by:\n",
    "- Stanley Nathanael Wijaya - 2702217125\n",
    "\n",
    "<code> Striving for Excellence ‚ù§Ô∏è‚Äçüî•‚ù§Ô∏è‚Äçüî• </code>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
