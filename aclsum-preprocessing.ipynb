{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <b>ACLSum Text Summarizer - Data Preprocessing and Vectorizer</b>\n",
    "Dataset Source: https://huggingface.co/datasets/sobamchan/aclsum\n",
    "<br>\n",
    "Docs Documentation: https://docs.google.com/document/d/1qSS2kVPMKn032hhjPmrgMquIb6Q827EiowY9y0s_k3I/edit?usp=sharing\n",
    "<br><br>\n",
    "<b>Stanley Nathanael Wijaya - 2702217125</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task Description\n",
    "\n",
    "As a data scientist in a tech company, you are tasked with developing a music recommendation system similar to Spotify’s.\n",
    "Dataset:\n",
    "<br>\n",
    "(https://www.kaggle.com/datasets/bricevergnou/spotify-recommendation)\n",
    "<br><br>\n",
    "This system aims to enhance user experience by suggesting songs or artists based on their listening history, preferences, and behavior.\n",
    "<ul>\n",
    "    <li>Choose an appropriate model for your recommendation system. This could be a collaborative filtering model, a content-based model, or even a hybrid model.</li>\n",
    "    <li>Develop the model to analyze user behavior and predict songs or artists they might like.</li>\n",
    "    <li>Determine how you will measure the success of your recommendation system. </li>\n",
    "    <li>Test the model with a set of users or simulated data to evaluate its performance.</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First Try"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Libary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Library needed to completely run the project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "import string\n",
    "import torch\n",
    "import pickle\n",
    "import numpy as np\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, T5ForConditionalGeneration\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Cleaning, Data Preprocessing, and Create Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ASUS\\xStyNWx\\Documents\\BINUS University\\Academic Courses\\Semester 4\\Natural Language Processing\\Project\\Repo\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\ASUS\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\ASUS\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\ASUS\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "Map: 100%|██████████| 100/100 [00:00<00:00, 272.25 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Model, Tokenizer, dan TF-IDF Vectorizer berhasil disimpan di folder 'saved_preprocessing_model/'\n"
     ]
    }
   ],
   "source": [
    "# Download NLP resources\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Initialize Lemmatizer & Stopwords\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Load dataset ACLSum\n",
    "dataset = load_dataset(\"sobamchan/aclsum\", split=\"train\")\n",
    "\n",
    "# Function untuk membersihkan teks\n",
    "def clean_text(text):\n",
    "    text = text.lower()  # Lowercasing\n",
    "    text = re.sub(r'\\d+', '', text)  # Menghapus angka\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))  # Menghapus tanda baca\n",
    "    words = word_tokenize(text)  # Tokenisasi\n",
    "    words = [lemmatizer.lemmatize(word) for word in words if word not in stop_words]  # Lemmatization & stopword removal\n",
    "    return \" \".join(words)\n",
    "\n",
    "# Preprocessing seluruh dataset\n",
    "for entry in dataset:\n",
    "    entry['document'] = clean_text(entry['document'])\n",
    "    entry['outcome'] = clean_text(entry['outcome'])\n",
    "\n",
    "# Inisialisasi tokenizer T5\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"t5-small\")\n",
    "\n",
    "# Function untuk tokenisasi dengan padding dan truncation\n",
    "def preprocess_data(example):\n",
    "    inputs = tokenizer(example['document'], max_length=1024, truncation=True, padding=\"max_length\")\n",
    "    labels = tokenizer(example['outcome'], max_length=150, truncation=True, padding=\"max_length\")\n",
    "    inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return inputs\n",
    "\n",
    "# Tokenisasi dataset\n",
    "tokenized_dataset = dataset.map(preprocess_data, batched=True)\n",
    "\n",
    "# Inisialisasi model T5\n",
    "model = T5ForConditionalGeneration.from_pretrained(\"t5-small\")\n",
    "\n",
    "# Menggunakan TF-IDF Vectorizer sebagai tambahan fitur\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=5000)  # Mengambil 5000 fitur paling penting\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform([entry['document'] for entry in dataset])\n",
    "\n",
    "# Simpan model, tokenizer, dan TF-IDF vectorizer setelah training\n",
    "model.save_pretrained(\"saved_preprocessing_model\")\n",
    "tokenizer.save_pretrained(\"saved_preprocessing_model\")\n",
    "\n",
    "# Simpan TF-IDF vectorizer menggunakan pickle\n",
    "with open(\"saved_preprocessing_model/tfidf_vectorizer.pkl\", \"wb\") as file:\n",
    "    pickle.dump(tfidf_vectorizer, file)\n",
    "\n",
    "print(\"✅ Model, Tokenizer, dan TF-IDF Vectorizer berhasil disimpan di folder 'saved_preprocessing_model/'\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Model and Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Model, Tokenizer, dan TF-IDF Vectorizer berhasil dimuat kembali!\n"
     ]
    }
   ],
   "source": [
    "# Load model yang telah disimpan\n",
    "model = T5ForConditionalGeneration.from_pretrained(\"saved_preprocessing_model\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"saved_preprocessing_model\")\n",
    "\n",
    "# Load TF-IDF Vectorizer\n",
    "with open(\"saved_preprocessing_model/tfidf_vectorizer.pkl\", \"rb\") as file:\n",
    "    tfidf_vectorizer = pickle.load(file)\n",
    "\n",
    "print(\"✅ Model, Tokenizer, dan TF-IDF Vectorizer berhasil dimuat kembali!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Testing Part 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Original Paper (Cleaned):\n",
      " In this paper , we explore correlation of dependency relation paths to rank candidate answers in answer extraction . Using the correlation measure , we compare dependency relations of a candidate answer and mapped question phrases in sentence with the corresponding relations in question . Different from previous studies , we propose an approximate phrase mapping algorithm and incorporate the mapping score into the correlation measure . The correlations are further incorporated into a Maximum Entropy-based ranking model which estimates path weights from training . Experimental results show that our method significantly outperforms state-ofthe-art syntactic relation-based methods by up to 20 % in MRR . Answer Extraction is one of basic modules in open domain Question Answering ( QA ) . It is to further process relevant sentences extracted with Passage / Sentence Retrieval and pinpoint exact answers using more linguistic-motivated analysis . Since QA turns to find exact answers rather than text snippets in recent years , answer extraction becomes more and more crucial . Typically , answer extraction works in the following steps : • Recognize expected answer type of a question . • Annotate relevant sentences with various types of named entities . • Regard the phrases annotated with the expected answer type as candidate answers . • Rank candidate answers . In the above work flow , answer extraction heavily relies on named entity recognition ( NER ) . On one hand , NER reduces the number of candidate answers and eases answer ranking . On the other hand , the errors from NER directly degrade answer extraction performance . To our knowledge , most top ranked QA systems in TREC are supported by effective NER modules which may identify and classify more than 20 types of named entities ( NE ) , such as abbreviation , music , movie , etc . However , developing such named entity recognizer is not trivial . Up to now , we have n't found any paper relevant to QA-specific NER development . So , it is hard to follow their work . In this paper , we just use a general MUC-based NER , which makes our results reproducible . A general MUC-based NER ca n't annotate a large number of NE classes . In this case , all noun phrases in sentences are regarded as candidate answers , which makes candidate answer sets much larger than those filtered by a well developed NER . The larger candidate answer sets result in the more difficult answer extraction . Previous methods working on surface word level , such as density-based ranking and pattern matching , may not perform well . Deeper linguistic analysis has to be conducted . This paper proposes a statistical method which exploring correlation of dependency relation paths to rank candidate answers . It is motivated by the observation that relations between proper answers and question phrases in candidate sentences are always similar to the corresponding relations in question . For example , the question \" What did Alfred Nobel invent ? \" and the candidate sentence \" ... in the will of Swedish industrialist Alfred Nobel , who invented dynamite . \" For each question , firstly , dependency relation paths are defined and extracted from the question and each of its candidate sentences . Secondly , the paths from the question and the candidate sentence are paired according to question phrase mapping score . Thirdly , correlation between two paths of each pair is calculated by employing Dynamic Time Warping algorithm . The input of the calculation is correlations between dependency relations , which are estimated from a set of training path pairs . Lastly , a Maximum Entropy-based ranking model is proposed to incorporate the path correlations and rank candidate answers . Furthermore , sentence supportive measure are presented according to correlations of relation paths among question phrases . It is applied to re-rank the candidate answers extracted from the different candidate sentences . Considering phrases may provide more accurate information than individual words , we extract dependency relations on phrase level instead of word level . The experiment on TREC questions shows that our method significantly outperforms a densitybased method by 50 % in MRR and three stateof-the-art syntactic-based methods by up to 20 % in MRR . Furthermore , we classify questions by judging whether NER is used . We investigate how these methods perform on the two question sets . The results indicate that our method achieves better performance than the other syntactic-based methods on both question sets . Especially for more difficult questions , for which NER may not help , our method improves MRR by up to 31 % . The paper is organized as follows . Section 2 discusses related work and clarifies what is new in this paper . Section 3 presents relation path correlation in detail . Section 4 and 5 discuss how to incorporate the correlations for answer ranking and re-ranking . Section 6 reports experiment and results . In this paper , we propose a relation path correlation-based method to rank candidate answers in answer extraction . We extract and pair relation paths from questions and candidate sentences . Next , we measure the relation path correlation in each pair based on approximate phrase mapping score and relation sequence alignment , which is calculated by DTW algorithm . Lastly , a ME-based ranking model is proposed to incorporate the path correlations and rank candidate answers . The experiment on TREC questions shows that our method significantly outperforms a density-based method by 50 % in MRR and three state-of-the-art syntactic-based methods by up to 20 % in MRR . Furthermore , the method is especially effective for difficult questions , for which NER may not help . Therefore , it may be used to further enhance state-of-the-art QA systems even if they have a good NER . In the future , we are to further evaluate the method based on the overall performance of a QA system and adapt it to sentence retrieval task .\n",
      "\n",
      "Generated Summary:\n",
      " . The results show that our method significantly outperforms a density-based method by 50 % in MRR. NER reduces the number of candidate answers and eases answer ranking.\n",
      "\n",
      "TF-IDF Features for Sample Paper:\n",
      " [0. 0. 0. ... 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "# Function untuk membuat ringkasan dengan model yang telah disimpan\n",
    "def generate_summary(text):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", max_length=1024, truncation=True)\n",
    "    summary_ids = model.generate(inputs.input_ids, max_length=150, min_length=40, length_penalty=2.0, num_beams=4, early_stopping=True)\n",
    "    return tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "\n",
    "# Contoh penggunaan dengan dokumen yang sudah diproses\n",
    "sample_paper = dataset[0]['document']\n",
    "print(\"\\nOriginal Paper (Cleaned):\\n\", sample_paper)\n",
    "print(\"\\nGenerated Summary:\\n\", generate_summary(sample_paper))\n",
    "\n",
    "# Menggunakan TF-IDF Vectorizer untuk representasi numerik teks\n",
    "tfidf_features = tfidf_vectorizer.transform([sample_paper])\n",
    "print(\"\\nTF-IDF Features for Sample Paper:\\n\", tfidf_features.toarray()[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Testing Part 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Original Paper (Cleaned):\n",
      " Automatically extracting social meaning and intention from spoken dialogue is an important task for dialogue systems and social computing . We describe a system for detecting elements of interactional style : whether a speaker is awkward , friendly , or flirtatious . We create and use a new spoken corpus of 991 4-minute speed-dates . Participants rated their interlocutors for these elements of style . Using rich dialogue , lexical , and prosodic features , we are able to detect flirtatious , awkward , and friendly styles in noisy natural conversational data with up to 75 % accuracy , compared to a 50 % baseline . We describe simple ways to extract relatively rich dialogue features , and analyze which features performed similarly for men and women and which were gender-specific . How can we extract social meaning from speech , deciding if a speaker is particularly engaged in the conversation , is uncomfortable or awkward , or is particularly friendly and flirtatious ? Understanding these meanings and how they are signaled in language is an important sociolinguistic task in itself . Extracting them automatically from dialogue speech and text is crucial for developing socially aware computing systems for tasks such as detection of interactional problems or matching conversational style , and will play an important role in creating more natural dialogue agents ( Pentland , 2005 ; Nass and Brave , 2005 ; Brave et al . , 2005 ) . Cues for social meaning permeate speech at every level of linguistic structure . Acoustic cues such as low and high F0 or energy and spectral tilt are important in detecting emotions such as annoyance , anger , sadness , or boredom ( Ang et al . , 2002 ; Lee and Narayanan , 2002 ; Liscombe et al . , 2003 ) , speaker characteristics such as charisma ( Rosenberg and Hirschberg , 2005 ) , or personality features like extroversion ( Mairesse et al . , 2007 ; Mairesse and Walker , 2008 ) . Lexical cues to social meaning abound . Speakers with links to depression or speakers who are under stress use more first person singular pronouns ( Rude et al . , 2004 ; Pennebaker and Lay , 2002 ; Cohn et al . , 2004 ) , positive emotion words are cues to agreeableness ( Mairesse et al . , 2007 ) , and negative emotion words are useful cues to deceptive speech ( Newman et al . , 2003 ) . The number of words in a sentence can be a useful feature for extroverted personality ( Mairesse et al . , 2007 ) . Finally , dialog features such as the presence of disfluencies can inform listeners about speakers ' problems in utterance planning or about confidence ( Brennan and Williams , 1995 ; Brennan and Schober , 2001 ) . Our goal is to see whether cues of this sort are useful in detecting particular elements of conversational style and social intention ; whether a speaker in a speed-dating conversation is judged by the interlocutor as friendly , awkward , or flirtatious . The results presented here should be regarded with some caution . The sample is not a random sample of English speakers or American adults , and speed dating is not a natural context for expressing every conversational style . Therefore , a wider array of studies across populations and genres would be required before a more general theory of conversational styles is established . On the other hand , the presented results may under-reflect the relations being captured . The quality of recordings and coarse granularity ( 1 second ) of the time-stamps likely cloud the relations , and as the data is cleaned and improved , we expect the associations to only grow stronger . Caveats aside , we believe the evidence indicates that the perception of several types of conversational style have relatively clear signals across genders , but with some additional gender contextualization . Both genders convey flirtation by laughing more , speaking faster , and using higher and more variable pitch . Both genders convey friendliness by laughing more , and using collaborative completions . However , we do find gender differences ; men asl more questions when ( labeled as ) flirting , women ask fewer . Men labeled as flirting are softer , but women labeled as flirting are louder . Women flirt-ing swear more , while men are more likely to use sexual vocabulary . Gender differences exist as well for the other variables . Men labeled as friendly use you while women labeled as friendly use I. Friendly women are very disfluent ; friendly men are not . While the features for friendly and flirtatious speech overlap , there are clear differences . Men speaker faster and with higher f0 ( min ) in flirtatious speech , but not faster and with lower f0 ( min ) in friendly speech . For men , flirtatious speech involves more questions and repair questions , while friendly speech does not . For women , friendly speech is more disfluent than flirtatious speech , and has more collaborative style ( completions , repair questions , appreciations ) . We also seem to see a model of collaborative conversational style ( probably related to the collaborative floor of Edelsky ( 1981 ) and Coates ( 1996 ) ) , cued by the use of more collaborative completions , repair questions and other questions , you , and laughter . These collaborative techniques were used by both women and men who were labeled as friendly , and occurred less with men labeled as awkward . Women themselves displayed more of this collaborative conversational style when they labeled the men as friendly . For women only , collaborative style included appreciations ; while for men only , collaborative style included overlaps . In addition to these implications for social science , our work has implications for the extraction of meaning in general . A key focus of our work was on ways to extract useful dialog act and disfluency features ( repair questions , backchannels , appreciations , restarts , dispreferreds ) with very shallow methods . These features were indeed extractable and proved to be useful features in classification . We are currently extending these results to predict date outcomes including ' liking ' , extending work such as Madan and Pentland ( 2006 ) .\n",
      "\n",
      "Generated Summary:\n",
      " . We create a new spoken corpus of 991 4-minute speed-dates. Participants rated their interlocutors for these elements of style. Using rich dialogue, lexical, and prosodic features, we are able to detect flirtatious, awkward, and friendly styles.\n",
      "\n",
      "TF-IDF Features for Sample Paper:\n",
      " [0. 0. 0. ... 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "# Function untuk membuat ringkasan dengan model yang telah disimpan\n",
    "def generate_summary(text):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", max_length=1024, truncation=True)\n",
    "    summary_ids = model.generate(inputs.input_ids, max_length=150, min_length=40, length_penalty=2.0, num_beams=4, early_stopping=True)\n",
    "    return tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "\n",
    "# Contoh penggunaan dengan dokumen yang sudah diproses\n",
    "sample_paper = dataset[10]['document']\n",
    "print(\"\\nOriginal Paper (Cleaned):\\n\", sample_paper)\n",
    "print(\"\\nGenerated Summary:\\n\", generate_summary(sample_paper))\n",
    "\n",
    "# Menggunakan TF-IDF Vectorizer untuk representasi numerik teks\n",
    "tfidf_features = tfidf_vectorizer.transform([sample_paper])\n",
    "print(\"\\nTF-IDF Features for Sample Paper:\\n\", tfidf_features.toarray()[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Testing Part 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Paper:\n",
      " In this paper , we explore correlation of dependency relation paths to rank candidate answers in answer extraction . Using the correlation measure , we compare dependency relations of a candidate answer and mapped question phrases in sentence with the corresponding relations in question . Different from previous studies , we propose an approximate phrase mapping algorithm and incorporate the mapping score into the correlation measure . The correlations are further incorporated into a Maximum Entropy-based ranking model which estimates path weights from training . Experimental results show that our method significantly outperforms state-ofthe-art syntactic relation-based methods by up to 20 % in MRR . Answer Extraction is one of basic modules in open domain Question Answering ( QA ) . It is to further process relevant sentences extracted with Passage / Sentence Retrieval and pinpoint exact answers using more linguistic-motivated analysis . Since QA turns to find exact answers rather than text snippets in recent years , answer extraction becomes more and more crucial . Typically , answer extraction works in the following steps : • Recognize expected answer type of a question . • Annotate relevant sentences with various types of named entities . • Regard the phrases annotated with the expected answer type as candidate answers . • Rank candidate answers . In the above work flow , answer extraction heavily relies on named entity recognition ( NER ) . On one hand , NER reduces the number of candidate answers and eases answer ranking . On the other hand , the errors from NER directly degrade answer extraction performance . To our knowledge , most top ranked QA systems in TREC are supported by effective NER modules which may identify and classify more than 20 types of named entities ( NE ) , such as abbreviation , music , movie , etc . However , developing such named entity recognizer is not trivial . Up to now , we have n't found any paper relevant to QA-specific NER development . So , it is hard to follow their work . In this paper , we just use a general MUC-based NER , which makes our results reproducible . A general MUC-based NER ca n't annotate a large number of NE classes . In this case , all noun phrases in sentences are regarded as candidate answers , which makes candidate answer sets much larger than those filtered by a well developed NER . The larger candidate answer sets result in the more difficult answer extraction . Previous methods working on surface word level , such as density-based ranking and pattern matching , may not perform well . Deeper linguistic analysis has to be conducted . This paper proposes a statistical method which exploring correlation of dependency relation paths to rank candidate answers . It is motivated by the observation that relations between proper answers and question phrases in candidate sentences are always similar to the corresponding relations in question . For example , the question \" What did Alfred Nobel invent ? \" and the candidate sentence \" ... in the will of Swedish industrialist Alfred Nobel , who invented dynamite . \" For each question , firstly , dependency relation paths are defined and extracted from the question and each of its candidate sentences . Secondly , the paths from the question and the candidate sentence are paired according to question phrase mapping score . Thirdly , correlation between two paths of each pair is calculated by employing Dynamic Time Warping algorithm . The input of the calculation is correlations between dependency relations , which are estimated from a set of training path pairs . Lastly , a Maximum Entropy-based ranking model is proposed to incorporate the path correlations and rank candidate answers . Furthermore , sentence supportive measure are presented according to correlations of relation paths among question phrases . It is applied to re-rank the candidate answers extracted from the different candidate sentences . Considering phrases may provide more accurate information than individual words , we extract dependency relations on phrase level instead of word level . The experiment on TREC questions shows that our method significantly outperforms a densitybased method by 50 % in MRR and three stateof-the-art syntactic-based methods by up to 20 % in MRR . Furthermore , we classify questions by judging whether NER is used . We investigate how these methods perform on the two question sets . The results indicate that our method achieves better performance than the other syntactic-based methods on both question sets . Especially for more difficult questions , for which NER may not help , our method improves MRR by up to 31 % . The paper is organized as follows . Section 2 discusses related work and clarifies what is new in this paper . Section 3 presents relation path correlation in detail . Section 4 and 5 discuss how to incorporate the correlations for answer ranking and re-ranking . Section 6 reports experiment and results . In this paper , we propose a relation path correlation-based method to rank candidate answers in answer extraction . We extract and pair relation paths from questions and candidate sentences . Next , we measure the relation path correlation in each pair based on approximate phrase mapping score and relation sequence alignment , which is calculated by DTW algorithm . Lastly , a ME-based ranking model is proposed to incorporate the path correlations and rank candidate answers . The experiment on TREC questions shows that our method significantly outperforms a density-based method by 50 % in MRR and three state-of-the-art syntactic-based methods by up to 20 % in MRR . Furthermore , the method is especially effective for difficult questions , for which NER may not help . Therefore , it may be used to further enhance state-of-the-art QA systems even if they have a good NER . In the future , we are to further evaluate the method based on the overall performance of a QA system and adapt it to sentence retrieval task .\n",
      "\n",
      "Generated Summary:\n",
      " In this paper, we explore correlation of dependency relation paths to rank candidate answers in answer extraction. Using the correlation measure, we compare dependency relations of a candidate answer and mapped question phrases in sentence with the corresponding relations in question.\n"
     ]
    }
   ],
   "source": [
    "# Function to generate summary\n",
    "def generate_summary(text):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", max_length=1024, truncation=True)\n",
    "    summary_ids = model.generate(inputs.input_ids, max_length=150, min_length=40, length_penalty=2.0, num_beams=4, early_stopping=True)\n",
    "    return tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "\n",
    "# Example usage\n",
    "sample_paper = dataset[0]['document']\n",
    "print(\"Original Paper:\\n\", sample_paper)\n",
    "print(\"\\nGenerated Summary:\\n\", generate_summary(sample_paper))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Evaluation using Rogue Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 100/100 [06:48<00:00,  4.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROUGE-1: 0.1928\n",
      "ROUGE-2: 0.0358\n",
      "ROUGE-L: 0.1375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from rouge_score import rouge_scorer\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, T5ForConditionalGeneration\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Load dataset ACLSum (gunakan data split yang tepat)\n",
    "dataset = load_dataset(\"sobamchan/aclsum\", split=\"train\")\n",
    "\n",
    "# Inisialisasi tokenizer dan model T5\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"t5-small\")\n",
    "model = T5ForConditionalGeneration.from_pretrained(\"saved_preprocessing_model\")  # Gunakan model yang telah disimpan\n",
    "\n",
    "# Function untuk generate summary\n",
    "def generate_summary(text):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", max_length=1024, truncation=True)\n",
    "    summary_ids = model.generate(inputs.input_ids, max_length=150, min_length=40, length_penalty=2.0, num_beams=4, early_stopping=True)\n",
    "    return tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "\n",
    "# Initialize the ROUGE scorer\n",
    "scorer = rouge_scorer.RougeScorer([\"rouge1\", \"rouge2\", \"rougeL\"], use_stemmer=True)\n",
    "\n",
    "# Define evaluation function\n",
    "def evaluate_model(dataset, num_samples=100):\n",
    "    all_rouge_scores = {\n",
    "        \"rouge1\": [],\n",
    "        \"rouge2\": [],\n",
    "        \"rougeL\": []\n",
    "    }\n",
    "\n",
    "    # Loop over samples\n",
    "    for i in tqdm(range(num_samples), desc=\"Evaluating\"):\n",
    "        # Extract document and reference summary\n",
    "        document = dataset[i]['document']\n",
    "        reference_summary = dataset[i]['outcome']\n",
    "        \n",
    "        # Generate summary using model\n",
    "        generated_summary = generate_summary(document)\n",
    "        \n",
    "        # Compute ROUGE score\n",
    "        scores = scorer.score(reference_summary, generated_summary)\n",
    "        \n",
    "        # Store the ROUGE scores\n",
    "        all_rouge_scores[\"rouge1\"].append(scores[\"rouge1\"].fmeasure)\n",
    "        all_rouge_scores[\"rouge2\"].append(scores[\"rouge2\"].fmeasure)\n",
    "        all_rouge_scores[\"rougeL\"].append(scores[\"rougeL\"].fmeasure)\n",
    "    \n",
    "    # Compute average ROUGE scores\n",
    "    avg_rouge1 = sum(all_rouge_scores[\"rouge1\"]) / len(all_rouge_scores[\"rouge1\"])\n",
    "    avg_rouge2 = sum(all_rouge_scores[\"rouge2\"]) / len(all_rouge_scores[\"rouge2\"])\n",
    "    avg_rougeL = sum(all_rouge_scores[\"rougeL\"]) / len(all_rouge_scores[\"rougeL\"])\n",
    "\n",
    "    return {\n",
    "        \"rouge1\": avg_rouge1,\n",
    "        \"rouge2\": avg_rouge2,\n",
    "        \"rougeL\": avg_rougeL\n",
    "    }\n",
    "\n",
    "# Evaluate the model on 100 samples (you can adjust the number of samples)\n",
    "evaluation_results = evaluate_model(dataset, num_samples=100)\n",
    "\n",
    "# Print the evaluation results\n",
    "print(f\"ROUGE-1: {evaluation_results['rouge1']:.4f}\")\n",
    "print(f\"ROUGE-2: {evaluation_results['rouge2']:.4f}\")\n",
    "print(f\"ROUGE-L: {evaluation_results['rougeL']:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Second Try - Seq2Seq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Library & Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\ASUS\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\ASUS\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\ASUS\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "import string\n",
    "import numpy as np\n",
    "import torch\n",
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from transformers import AutoTokenizer, T5ForConditionalGeneration\n",
    "from rouge_score import rouge_scorer\n",
    "\n",
    "# Download needed NLTK resources\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Initialize Lemmatizer & Stopwords\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words('english'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to clean and preprocess text\n",
    "def clean_text(text):\n",
    "    text = text.lower()  # Lowercase\n",
    "    text = re.sub(r'\\d+', '', text)  # Remove digits\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))  # Remove punctuation\n",
    "    words = word_tokenize(text)  # Tokenization\n",
    "    words = [lemmatizer.lemmatize(word) for word in words if word not in stop_words]  # Lemmatization & stopword removal\n",
    "    return \" \".join(words)\n",
    "\n",
    "# Load dataset ACLSum\n",
    "dataset = load_dataset(\"sobamchan/aclsum\", split=\"train\")\n",
    "\n",
    "# Clean and preprocess the 'document' and 'outcome' columns\n",
    "for entry in dataset:\n",
    "    entry['document'] = clean_text(entry['document'])\n",
    "    entry['outcome'] = clean_text(entry['outcome'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['id', 'document', 'challenge', 'approach', 'outcome'],\n",
      "    num_rows: 100\n",
      "})\n",
      "{'id': 'D08-1050', 'document': 'Most state-of-the-art wide-coverage parsers are trained on newspaper text and suffer a loss of accuracy in other domains , making parser adaptation a pressing issue . In this paper we demonstrate that a CCG parser can be adapted to two new domains , biomedical text and questions for a QA system , by using manually-annotated training data at the POS and lexical category levels only . This approach achieves parser accuracy comparable to that on newspaper data without the need for annotated parse trees in the new domain . We find that retraining at the lexical category level yields a larger performance increase for questions than for biomedical text and analyze the two datasets to investigate why different domains might behave differently for parser adaptation . Most state-of-the-art wide-coverage parsers are based on the Penn Treebank ( Marcus et al . , 1993 ) , making such parsers highly tuned to newspaper text . A pressing question facing the parsing community is how to adapt these parsers to other domains , such as biomedical research papers and web pages . A related question is how to improve the performance of these parsers on constructions that are rare in the Penn Treebank , such as questions . Questions are particularly important since a question parser is a component in most Question Answering ( QA ) systems ( Harabagiu et al . , 2001 ) . In this paper we investigate parser adaptation in the context of lexicalized grammars , by using a parser based on Combinatory Categorial Grammar ( CCG ) ( Steedman , 2000 ) . A key property of CCG is that it is lexicalized , meaning that each word in a sentence is associated with an elementary syntactic structure . In the case of CCG this is a lexical category expressing subcategorization information . We exploit this property of CCG by performing manual annotation in the new domain , but only up to this level of representation , where the annotation can be carried out relatively quickly . Since CCG lexical categories are so expressive , many of the syntactic characteristics of a domain are captured at this level . The two domains we consider are the biomedical domain and questions for a QA system . We use the term \" domain \" somewhat loosely here , since questions are best described as a particular set of syntactic constructions , rather than a set of documents about a particular topic . However , we consider question data to be interesting in the context of domain adaptation for the following reasons : 1 ) there are few examples in the Penn Treebank ( PTB ) and so PTB parsers typically perform poorly on them ; 2 ) questions form a fairly homogeneous set with respect to the syntactic constructions employed , and it is an interesting question how easy it is to adapt a parser to such data ; and 3 ) QA is becoming an important example of NLP technology , and question parsing is an important task for QA systems . The CCG parser we use ( Clark and Curran , 2007b ) makes use of three levels of representation : one , a POS tag level based on the fairly coarse-grained POS tags in the Penn Treebank ; two , a lexical category level based on the more fine-grained CCG lexical categories , which are assigned to words by a CCG su-pertagger ; and three , a hierarchical level consisting of CCG derivations . A key idea in this paper , following a pilot study in Clark et al . ( 2004 ) , is to perform manual annotation only at the first two levels . Since the lexical category level consists of sequences of tags , rather than hierarchical derivations , the annotation can be performed relatively quickly . For the biomedical and question domains we manually annotated approximately 1,000 and 2,000 sentences , respectively , with CCG lexical categories . We also created a gold standard set of grammatical relations ( GR ) in the Stanford format ( de Marneffe et al . , 2006 ) , using 500 of the questions . For the biomedical domain we used the BioInfer corpus ( Pyysalo et al . , 2007a ) , an existing gold-standard GR resource also in the Stanford format . We evaluated the parser on both lexical category assignment and recovery of GRs . The results show that the domain adaptation approach used here is successful in two very different domains , achieving parsing accuracy comparable to state-of-the-art accuracy for newspaper text . The results also show , however , that the two domains have different profiles with regard to the levels of representation used by the parser . We find that simply retraining the POS tagger used by the parser leads to a large improvement in performance for the biomedical domain , and that retraining the CCG supertagger on the annotated biomedical data improves the performance further . For the question data , retraining just the POS tagger also improves parser performance , but retraining the supertagger has a much greater effect . We perform some analysis of the two datasets in order to explain the different behaviours with regard to porting the CCG parser . We have targeted lower levels of representation in order to adapt a lexicalized-grammar parser to two new domains , biomedical text and questions . Although each of the lower levels has been targeted independently in previous work , this is the first study that examines both levels together to determine how they affect parsing accuracy . We achieved an accuracy on grammatical relations in the same range as that of the original parser for newspaper text , without requiring costly annotation of full parse trees . Both biomedical and question data are domains in which there is an immediate need for accurate parsing . The question dataset is in some ways an extreme example for domain adaptation , since the sentences are syntactically uniform ; on the other hand , it is of interest as a set of constructions where the parser initially performed poorly , and is a realistic parsing challenge in the context of QA systems . Interestingly , although an increase in accuracy at each stage of the pipeline did yield an increase at the following stage , these increases were not uniform across the two domains . The new POS tagger model was responsible for most of the improvement in parsing for the biomedical domain , while the new supertagger model was necessary to see a large improvement in the question domain . We attribute this to the fact that question syntax is significantly different from newspaper syntax . We expect these considerations to apply to any lexicalized-grammar parser . Of course , it would be useful to have a way of predicting which level of annotation would be most effective for adapting to a new domain before the annotation begins . The utility of measures such as unknown word rate ( which can be performed with unlabelled data ) and unknown POS n-gram rate ( which can be performed with only POS tags ) is not yet sufficiently clear to rely on them as predictive measures , but it seems a fruitful avenue for future work to investigate the importance of such measures for parser domain adaptation .', 'challenge': 'Most existing parsers are tuned for newspaper texts making them limited in applicable domains.', 'approach': 'They propose a method to adapt a CCG parser to new domains using manually-annotated data only at POS and lexical category levels.', 'outcome': 'The proposed method achieves comparable results to in-domain parsers without expensive full annotations on biomedical texts and questions that are rare in existing benchmark datasets.'}\n"
     ]
    }
   ],
   "source": [
    "# Print the structure of the dataset\n",
    "print(dataset)\n",
    "print(dataset[99])  # Check the first data entry\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filter: 100%|██████████| 100/100 [00:00<00:00, 3448.38 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# Drop any rows with missing 'document' or 'outcome' fields\n",
    "dataset = dataset.filter(lambda x: x['document'] is not None and x['outcome'] is not None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Data: 80 samples\n",
      "Validation Data: 10 samples\n",
      "Test Data: 10 samples\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Convert the dataset into a list of dictionaries for easier handling\n",
    "data_list = [entry for entry in dataset]\n",
    "\n",
    "# Split the data into training, validation, and test sets\n",
    "train_data, temp_data = train_test_split(data_list, test_size=0.2, random_state=42)  # 80% training, 20% temporary\n",
    "val_data, test_data = train_test_split(temp_data, test_size=0.5, random_state=42)  # 50% for validation and 50% for test\n",
    "\n",
    "print(f\"Training Data: {len(train_data)} samples\")\n",
    "print(f\"Validation Data: {len(val_data)} samples\")\n",
    "print(f\"Test Data: {len(test_data)} samples\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['id', 'document', 'challenge', 'approach', 'outcome'],\n",
      "    num_rows: 80\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "# Convert the data back into a Hugging Face Dataset format\n",
    "train_dataset = Dataset.from_pandas(pd.DataFrame(train_data))\n",
    "val_dataset = Dataset.from_pandas(pd.DataFrame(val_data))\n",
    "test_dataset = Dataset.from_pandas(pd.DataFrame(test_data))\n",
    "\n",
    "# Print to check\n",
    "print(train_dataset)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF Features:  (80, 5000)\n"
     ]
    }
   ],
   "source": [
    "# Initialize TF-IDF Vectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=5000)  # Limiting to top 5000 features (words)\n",
    "train_tfidf_matrix = tfidf_vectorizer.fit_transform([entry['document'] for entry in train_data])\n",
    "\n",
    "# TF-IDF representation of training documents\n",
    "print(\"TF-IDF Features: \", train_tfidf_matrix.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 80/80 [00:00<00:00, 373.54 examples/s]\n",
      "Map: 100%|██████████| 10/10 [00:00<00:00, 78.29 examples/s]\n",
      "Map: 100%|██████████| 10/10 [00:00<00:00, 238.48 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': 'N18-1108', 'document': 'Recurrent neural networks ( RNNs ) have achieved impressive results in a variety of linguistic processing tasks , suggesting that they can induce non-trivial properties of language . We investigate here to what extent RNNs learn to track abstract hierarchical syntactic structure . We test whether RNNs trained with a generic language modeling objective in four languages ( Italian , English , Hebrew , Russian ) can predict long-distance number agreement in various constructions . We include in our evaluation nonsensical sentences where RNNs can not rely on semantic or lexical cues ( \" The colorless green ideas ideas ideas ideas ideas ideas ideas ideas ideas ideas ideas ideas ideas ideas ideas ideas ideas I ate with the chair sleep sleep sleep sleep sleep sleep sleep sleep sleep sleep sleep sleep sleep sleep sleep sleep sleep furiously \" ) , and , for Italian , we compare model performance to human intuitions . Our language-model-trained RNNs make reliable predictions about long-distance agreement , and do not lag much behind human performance . We thus bring support to the hypothesis that RNNs are not just shallowpattern extractors , but they also acquire deeper grammatical competence . Recurrent neural networks ( RNNs ; Elman , 1990 ) are general sequence processing devices that do not explicitly encode the hierarchical structure that is thought to be essential to natural language ( Everaert et al . , 2015 ) . Early work using artificial languages showed that they may nevertheless be able to approximate context-free languages ( Elman , 1991 ) . More recently , RNNs have achieved impressive results in large-scale tasks such as language modeling for speech recognition and machine translation , and are by now standard tools for sequential natural language tasks ( e.g. , Mikolov et al . , 2010 ; Graves , 2012 ; Wu et al . , 2016 ) . This suggests that RNNs may learn to track grammatical structure even when trained on noisier natural data . The conjecture is supported by the success of RNNs as feature extractors for syntactic parsing ( e.g. , Cross and Huang , 2016 ; Kiperwasser and Goldberg , 2016 ; Zhang et al . , 2017 ) . Linzen et al . ( 2016 ) directly evaluated the extent to which RNNs can approximate hierarchical structure in corpus-extracted natural language data . They tested whether RNNs can learn to predict English subject-verb agreement , a task thought to require hierarchical structure in the general case ( \" the girl girl girl girl girl girl girl girl girl girl girl girl girl girl girl girl girl the boys like . Their experiments confirmed that RNNs can , in principle , handle such constructions . However , in their study RNNs could only succeed when provided with explicit supervision on the target task . Linzen and colleagues argued that the unsupervised language modeling objective is not sufficient for RNNs to induce the syntactic knowledge necessary to cope with long-distance agreement . . . is is is is is is is is is is is is is is is is is The current paper reevaluates these conclusions . We strengthen the evaluation paradigm of Linzen and colleagues in several ways . Most importantly , their analysis did not rule out the possibility that RNNs might be relying on semantic or collocational / frequency-based information , rather than purely on syntactic structure . In \" dogs dogs dogs dogs dogs about what typically barks ( dogs , not neighbourhoods ) , without relying on more abstract structural cues . In a follow-up study to Linzen and colleagues \\' , Bernardy and Lappin ( 2017 ) observed that RNNs are better at long-distance agreement when they construct rich lexical representations of words , which suggests effects of this sort might indeed be at play . We introduce a method to probe the syntactic abilities of RNNs that abstracts away from potential lexical , semantic and frequency-based confounds . Inspired by Chomsky \\'s ( 1957 ) We extend the previous work in three additional ways . First , alongside English , which has few morphological cues to agreement , we examine Italian , Hebrew and Russian , which have richer morphological systems . Second , we go beyond subject-verb agreement and develop an automated method to harvest a variety of long-distance number agreement constructions from treebanks . Finally , for Italian , we collect human judgments for the tested sentences , providing an important comparison point for RNN performance . 1We focus on the more interesting unsupervised setup , where RNNs are trained to perform generic , large-scale language modeling ( LM ): they are not given explicit evidence , at training time , that they must focus on long-distance agreement , but they are rather required to track a multitude of cues that might help with word prediction in general . Our results are encouraging . RNNs trained with a LM objective solve the long-distance agreement problem well , even on nonce sentences . The pattern is consistent across languages , and , crucially , not far from human performance in Italian . Moreover , RNN performance on language modeling ( measured in terms of perplexity ) is a good predictor of long-distance agreement accuracy . This suggests that the ability to capture structural generalizations is an important aspect of what makes the best RNN architectures so good at language modeling . Since our positive results contradict , to some extent , those of Linzen et al . ( 2016 ) , we also replicate their relevant experiment using our best RNN ( an LSTM ) . We outperform their models , suggesting that a careful architecture / hyperparameter search is crucial to obtain RNNs that are not only good at language modeling , but able to extract syntactic generalizations . 2 Constructing a long-distance agreement benchmark Overview . We construct our number agreement test sets as follows . Original sentences are automatically extracted from a dependency treebank . They are then converted into nonce sentences by substituting all content words with random words with the same morphology , resulting in grammatical but nonsensical sequences . An LM is evaluated on its predictions for the target ( second ) word in the dependency , in both the original and nonce sentences . Long-distance agreement constructions . Agreement relations , such as subject-verb agreement in English , are an ideal test bed for the syntactic abilities of LMs , because the form of the second item ( the target ) is predictable from the first item ( the cue ) . Crucially , the cue and the target are linked by a structural relation , where linear order in the word sequence does not matter ( Everaert et al . , 2015 ) In all these cases , the number of the main verb \" thinks \" is determined by its subject ( \" girl \" ) , and this relation depends on the syntactic structure of the sentence , not on the linear sequence of words . As the last sentence shows , the word directly preceding the verb can even be a noun with the opposite number ( \" friends \" ) , but this does not influence the structurally-determined form of the verb . When the cue and the target are adjacent ( \" the girl girl girl girl girl girl girl girl girl girl girl girl girl girl girl girl girl thinks thinks thinks thinks thinks thinks thinks thinks thinks thinks thinks thinks thinks thinks thinks thinks thinks . . . \" ) , an LM can predict the target without access to syntactic structure : it can simply extract the relevant morphosyntactic features of words ( e.g. , number ) and record the co-occurrence frequencies of patterns such as N P lur V P lur ( Mikolov et al . , 2013 ) . Thus , we focus here on long-distance agreement , where an arbitrary num- ber of words can occur between the elements of the agreement relation . We limit ourselves to number agreement ( plural or singular ) , as it is the only overt agreement feature shared by all of the languages we study . Identifying candidate constructions . We started by collecting pairs of part-of-speech ( POS ) tags connected by a dependency arc . Independently of which element is the head of the relation , we refer to the first item as the cue and to the second as the target . We additionally refer to the POS sequence characterizing the entire pattern as a construction , and to the elements in the middle as context . For each candidate construction , we collected all of the contexts in the corpus that intervene between the cue and the target ( we define contexts as the sequence of POS tags of the top-level nodes in the dependency subtrees ) . For example , for the English subject-verb agreement construction shown in Fig . 1a , the context is defined by VERB ( head of the relative clause ) and ADV ( adverbial modifier of the target verb ) , which together dominate the sequence \" the boys like often \" . For the Russian adjective-noun agreement construction in Fig . 1b , the context is NOUN , because in the dependency grammar we use the noun \" moment \" is the head of the prepositional phrase \" at that moment \" , which modifies the adjective \" deep \" . The candidate agreement pair and the context form a construction , which is characterized by a sequence of POS tags , e.g. , NOUN VERB ADV VERB or VERB NOUN CCONJ VERB ( Fig . 1c ) . is is is is is is is is is is is is is is is is is \" and \" girls girls girls girls girls girls girls girls girls girls girls girls girls girls girls girls girls who stayed at home were were were were were were were were were were were were were were were were were \" . Conversely , standard syntactic structures might be split between different constructions , e.g. , relative clause contexts occur in both NOUN VERB VERB and NOUN VERB ADV VERB constructions ( the latter is illustrated by the English example in Fig . 1a ) . Construction contexts can contain a variable numbers of words . Since we are interested in challenging cases , we only considered cases in which at least three tokens intervened between the cue and the target . Excluding non-agreement constructions . In the next step , we excluded constructions in which the candidate cue and target did not agree in number in all of the instances of the construction in the treebank ( if both the cue and the target were morphologically annotated for number ) . This step retained English subject-verb constructions , for example , but excluded verb-object constructions , since any form of a verb can appear both with singular and plural objects . To focus on robust agreement patterns , we only kept constructions with at least 10 instances of both plural and singular agreement . When applied to the treebanks we used ( see Section 3 ) , this step resulted in between two ( English ) and 21 ( Russian ) constructions per lan-guage . English has the poorest morphology and consequently the lowest number of patterns with identifiable morphological agreement . Only the VP-conjunction construction ( Fig . 1c ) was identified in all four languages . Subject-verb agreement constructions were extracted in all languages but Russian ; Russian has relatively flexible word order and a noun dependent preceding a head verb is not necessarily its subject . The full list of extracted constructions in English and Italian is given in Tables 2 and 3 , respectively . For the other languages , see the Supplementary Material ( SM ) . 2 Original sentence test set . Our \" original \" sentence test set included all sentences from each construction where all words from the cue and up to and including the target occurred in the LM vocabulary ( Section 3 ) , and where the singular / plural counterpart of the target occurred in the treebank and in the language model vocabulary ( this is required by the evaluation procedure outlined below ) . The total counts of constructions and original sentences in our test sets are provided in Table 1 . The average number of context words separating the cue and the target ranged from 3.6 ( Hebrew ) to 4.5 ( Italian ) . Generating nonce sentences . We generated nine nonce variants of each original sentence as follows . Each content word ( noun , verb , adjective , proper noun , numeral , adverb ) in the sentence was substituted by another random content word from the treebank with matching POS and morphological features . To avoid forms that are ambiguous between several POS , which are particularly frequent in English ( e.g. , plural noun and singular verb forms ) , we excluded the forms that appeared with a different POS more than 10 % of the time in the treebank . Function words ( determiners , pronouns , adpositions , particles ) and punctuation were left intact . For example , we generated the nonce ( 1b ) from the original sentence ( 1a ): ( ( e.g. , \" it stays the shuttle \" in ( 1b ) ) . Evaluation procedure . For each sentence in our test set , we retrieved from our treebank the form that is identical to the agreement target in all morphological features except number ( e.g. , \" finds \" instead of \" find \" in ( 1b ) ) . Given a sentence with prefix p up to and excluding the target , we then compute the probabilities P ( t 1 |p ) and P ( t 2 |p ) for the singular and plural variants of the target , t 1 and t 2 , based on the language model . Following Linzen et al . ( 2016 ) , we say that the model identified the correct target if it assigned a higher probability to the form with the correct number . In ( 1b ) , for example , the model should assign a higher probability to \" finds \" than \" find\".3 3 Experimental setup Treebanks . We extracted our test sets from the Italian , English , Hebrew and Russian Universal Dependency treebanks ( UD , v2.0 , Nivre et al . , 2016 ) . The English and Hebrew treebanks were post-processed to obtain a richer morphological annotation at the word level ( see SM for details ) . We ran an extensive analysis of the abilities of RNNs trained on a generic language-modeling task to predict long-distance number agreement . Results were consistent across four languages and a number of constructions . They were above strong baselines even in the challenging case of nonsense sentences , and not far from human performance . We are not aware of other collections of human long-distance agreement judgments on nonsensical sentences , and we thus consider our publicly available data set an important contribution of our work , of interest to students of human language processing in general . The constructions we considered are quite infrequent ( according to a rough estimate based on the treebanks , the language in which they are most common is Hebrew , and even there they occur with average 0.8 % sentence frequency ) . Moreover , they vary in the contexts that separate the cue and the target . So , RNNs are not simply memorizing frequent morphosyntactic sequences ( which would already be impressive , for systems learning from raw text ) . We tentatively conclude that LMtrained RNNs can construct abstract grammatical representations of their input . This , in turn , suggests that the input itself contains enough information to trigger some form of syntactic learning in a system , such as an RNN , that does not contain an explicit prior bias in favour of syntactic structures . In future work , we would like to better understand what kind of syntactic information RNNs are encoding , and how . On the one hand , we plan to adapt methods to inspect information flow across RNN states ( e.g. , Hupkes et al . , 2017 ) . On the other , we would like to expand our empirical investigation by focusing on other long-distance phenomena , such as overt case assignment ( Blake , 2001 ) or parasitic gap licensing ( Culicover and Postal , 2001 ) . While it is more challenging to extract reliable examples of such phenomena from corpora , their study would probe more sophisticated syntactic capabilities , possibly even shedding light on the theoretical analysis of the underlying linguistic structures . Finally , it may be useful to complement the corpus-driven approach used in the current paper with constructed evaluation sentences that isolate particular syntactic phenomena , independent of their frequency in a natural corpus , as is common in psycholinguistics ( Enguehard et al . , 2017 ) .', 'challenge': 'Previous work only shows that RNNs can handle constructions that require hierarchical structure when explicit supervision on the target task is given.', 'approach': 'They introduce a probing method for syntactic abilities to evaluate long-distance agreement on standard and nonsensical sentences in multiple languages with different morphological systems.', 'outcome': 'The RNNs trained on an LM objective can solve long-distance agreement problems well even on nonsensical sentences consistently across languages indicating their deeper grammatical competence.', 'input_ids': [419, 14907, 24228, 5275, 41, 391, 17235, 7, 3, 61, 43, 5153, 4423, 772, 16, 3, 9, 1196, 13, 3, 24703, 3026, 4145, 3, 6, 15495, 24, 79, 54, 21151, 529, 18, 1788, 5907, 40, 2605, 13, 1612, 3, 5, 101, 9127, 270, 12, 125, 5996, 391, 17235, 7, 669, 12, 1463, 9838, 1382, 7064, 1950, 8953, 17, 2708, 447, 1809, 3, 5, 101, 794, 823, 391, 17235, 7, 4252, 28, 3, 9, 8165, 1612, 15309, 5997, 16, 662, 8024, 41, 4338, 3, 6, 1566, 3, 6, 20428, 3, 6, 4263, 3, 61, 54, 9689, 307, 18, 26, 23, 8389, 381, 2791, 16, 796, 1449, 7, 3, 5, 101, 560, 16, 69, 5002, 529, 7, 35, 7, 1950, 16513, 213, 391, 17235, 7, 54, 59, 3, 4610, 30, 27632, 42, 3, 30949, 138, 123, 15, 7, 41, 96, 37, 945, 924, 1442, 912, 912, 912, 912, 912, 912, 912, 912, 912, 912, 912, 912, 912, 912, 912, 912, 912, 27, 3, 342, 28, 8, 3533, 2085, 2085, 2085, 2085, 2085, 2085, 2085, 2085, 2085, 2085, 2085, 2085, 2085, 2085, 2085, 2085, 2085, 4223, 23, 11937, 96, 3, 61, 3, 6, 11, 3, 6, 21, 4338, 3, 6, 62, 4048, 825, 821, 12, 936, 26207, 7, 3, 5, 421, 1612, 18, 21770, 18, 17, 10761, 391, 17235, 7, 143, 3468, 20099, 81, 307, 18, 26, 23, 8389, 2791, 3, 6, 11, 103, 59, 3, 5430, 231, 1187, 936, 821, 3, 5, 101, 2932, 830, 380, 12, 8, 22455, 24, 391, 17235, 7, 33, 59, 131, 16906, 4665, 2947, 5819, 127, 7, 3, 6, 68, 79, 92, 7464, 7231, 3, 5096, 4992, 138, 22368, 3, 5, 419, 14907, 24228, 5275, 41, 391, 17235, 7, 3, 117, 1289, 348, 3, 6, 5541, 3, 61, 33, 879, 5932, 3026, 1904, 24, 103, 59, 21119, 23734, 8, 1382, 7064, 1950, 1809, 24, 19, 816, 12, 36, 1832, 12, 793, 1612, 41, 6381, 9, 49, 17, 3, 15, 17, 491, 3, 5, 3, 6, 1230, 3, 61, 3, 5, 8840, 161, 338, 7353, 8024, 3217, 24, 79, 164, 17516, 36, 3, 179, 12, 24672, 2625, 18, 2113, 8024, 41, 1289, 348, 3, 6, 9957, 3, 61, 3, 5, 1537, 1310, 3, 6, 391, 17235, 7, 43, 5153, 4423, 772, 16, 508, 18, 6649, 4145, 224, 38, 1612, 15309, 21, 5023, 5786, 11, 1437, 7314, 3, 6, 11, 33, 57, 230, 1068, 1339, 21, 29372, 793, 1612, 4145, 41, 3, 15, 5, 122, 5, 3, 6, 21475, 32, 5850, 3, 15, 17, 491, 3, 5, 3, 6, 2735, 3, 117, 15199, 15, 7, 3, 6, 1673, 3, 117, 17792, 3, 15, 17, 491, 3, 5, 3, 6, 1421, 3, 61, 3, 5, 100, 6490, 24, 391, 17235, 7, 164, 669, 12, 1463, 3, 5096, 4992, 138, 1809, 237, 116, 4252, 30, 1746, 7, 972, 793, 331, 3, 5, 37, 975, 11827, 1462, 19, 3510, 57, 8, 1269, 13, 391, 17235, 7, 38, 1451, 5819, 127, 7, 21, 8953, 17, 2708, 447, 260, 7, 53, 41, 3, 15, 5, 122, 5, 3, 6, 4737, 11, 29034, 3, 6, 1421, 3, 117, 4320, 883, 9292, 11, 2540, 2235, 3, 6, 1421, 3, 117, 28015, 3, 15, 17, 491, 3, 5, 3, 6, 1233, 3, 61, 3, 5, 6741, 1847, 3, 15, 17, 491, 3, 5, 41, 1421, 3, 61, 1461, 14434, 8, 5996, 12, 84, 391, 17235, 7, 54, 24672, 1382, 7064, 1950, 1809, 16, 11736, 302, 18, 994, 11674, 793, 1612, 331, 3, 5, 328, 5285, 823, 391, 17235, 7, 54, 669, 12, 9689, 1566, 1426, 18, 11868, 2791, 3, 6, 3, 9, 2491, 816, 12, 1457, 1382, 7064, 1950, 1809, 16, 8, 879, 495, 41, 96, 8, 3202, 3202, 3202, 3202, 3202, 3202, 3202, 3202, 3202, 3202, 3202, 3202, 3202, 3202, 3202, 3202, 3202, 8, 5234, 114, 3, 5, 2940, 12341, 5899, 24, 391, 17235, 7, 54, 3, 6, 16, 8454, 3, 6, 2174, 224, 1449, 7, 3, 5, 611, 3, 6, 16, 70, 810, 391, 17235, 7, 228, 163, 7229, 116, 937, 28, 17623, 15520, 30, 8, 2387, 2491, 3, 5, 6741, 1847, 11, 6976, 3, 15585, 24, 8, 73, 23313, 1612, 15309, 5997, 19, 59, 6684, 21, 391, 17235, 7, 12, 21151, 8, 8953, 17, 2708, 447, 1103, 1316, 12, 14331, 28, 307, 18, 26, 23, 8389, 2791, 3, 5, 3, 5, 3, 5, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 37, 750, 1040, 3, 60, 15, 7480, 6203, 175, 18101, 3, 5, 101, 8726, 8, 5002, 20491, 13, 6741, 1847, 11, 6976, 16, 633, 1155, 3, 5, 1377, 7521, 3, 6, 70, 1693, 410, 59, 3356, 91, 8, 5113, 24, 391, 17235, 7, 429, 36, 3, 4610, 53, 30, 27632, 42, 7632, 14836, 138, 3, 87, 7321, 18, 390, 251, 3, 6, 1066, 145, 3, 18760, 30, 8953, 17, 2708, 447, 1809, 3, 5, 86, 96, 3887, 3887, 3887, 3887, 3887, 81, 125, 3115, 21696, 7, 41, 3887, 3, 6, 59, 19704, 7, 3, 61, 3, 6, 406, 3, 4610, 53, 30, 72, 9838, 8649, 123, 15, 7, 3, 5, 86, 3, 9, 1130, 18, 413, 810, 12, 6741, 1847, 11, 6976, 3, 31, 3, 6, 14735, 63, 11, 325, 1572, 77, 41, 1233, 3, 61, 6970, 24, 391, 17235, 7, 33, 394, 44, 307, 18, 26, 23, 8389, 2791, 116, 79, 6774, 2354, 3, 30949, 138, 6497, 7, 13, 1234, 3, 6, 84, 6490, 1951, 13, 48, 1843, 429, 5071, 36, 44, 577, 3, 5, 101, 4277, 3, 9, 1573, 12, 12732, 8, 8953, 17, 2708, 447, 8075, 13, 391, 17235, 7, 24, 9838, 7, 550, 45, 1055, 3, 30949, 138, 3, 6, 27632, 11, 7321, 18, 390, 975, 19732, 7, 3, 5, 22783, 57, 6285, 51, 5352, 3, 31, 7, 41, 24011, 3, 61, 101, 4285, 8, 1767, 161, 16, 386, 1151, 1155, 3, 5, 1485, 3, 6, 5815, 1566, 3, 6, 84, 65, 360, 3, 8886, 4478, 123, 15, 7, 12, 2791, 3, 6, 62, 5443, 4338, 3, 6, 20428, 11, 4263, 3, 6, 84, 43, 2354, 49, 3, 8886, 4478, 1002, 3, 5, 5212, 3, 6, 62, 281, 1909, 1426, 18, 11868, 2791, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': [37, 391, 17235, 7, 4252, 30, 46, 3, 11160, 5997, 54, 4602, 307, 18, 26, 23, 8389, 2791, 982, 168, 237, 30, 529, 7, 35, 7, 1950, 16513, 8182, 640, 8024, 3, 15716, 70, 7231, 3, 5096, 4992, 138, 22368, 5, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Function for tokenizing the dataset (same as before)\n",
    "def preprocess_data(example):\n",
    "    inputs = tokenizer(example['document'], max_length=1024, truncation=True, padding=\"max_length\")\n",
    "    labels = tokenizer(example['outcome'], max_length=150, truncation=True, padding=\"max_length\")\n",
    "    inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return inputs\n",
    "\n",
    "# Tokenize the training, validation, and test datasets\n",
    "tokenized_train_data = train_dataset.map(preprocess_data, batched=True)\n",
    "tokenized_val_data = val_dataset.map(preprocess_data, batched=True)\n",
    "tokenized_test_data = test_dataset.map(preprocess_data, batched=True)\n",
    "\n",
    "# Cek hasil tokenisasi\n",
    "print(tokenized_train_data[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_22624\\2166871522.py:17: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Seq2SeqTrainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='60' max='60' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [60/60 09:06, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>9.132276</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>5.899254</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>4.845070</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=60, training_loss=8.714350382486979, metrics={'train_runtime': 554.8254, 'train_samples_per_second': 0.433, 'train_steps_per_second': 0.108, 'total_flos': 64964064706560.0, 'train_loss': 8.714350382486979, 'epoch': 3.0})"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
    "\n",
    "# Define training arguments\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"./aclsum-results\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=3,\n",
    "    num_train_epochs=3,\n",
    "    predict_with_generate=True\n",
    ")\n",
    "\n",
    "# Initialize the trainer\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train_data,\n",
    "    eval_dataset=tokenized_val_data,\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the Fine-tuned Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('saved_model2\\\\tokenizer_config.json',\n",
       " 'saved_model2\\\\special_tokens_map.json',\n",
       " 'saved_model2\\\\tokenizer.json')"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save the fine-tuned model\n",
    "model.save_pretrained(\"saved_model2\")\n",
    "tokenizer.save_pretrained(\"saved_model2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the Saved Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import T5ForConditionalGeneration, AutoTokenizer\n",
    "\n",
    "# Load saved model\n",
    "dataset = load_dataset(\"sobamchan/aclsum\", split=\"train\")\n",
    "model = T5ForConditionalGeneration.from_pretrained(\"saved_model2\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"saved_model2\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Summarizer Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Paper:\n",
      " In this paper , we explore correlation of dependency relation paths to rank candidate answers in answer extraction . Using the correlation measure , we compare dependency relations of a candidate answer and mapped question phrases in sentence with the corresponding relations in question . Different from previous studies , we propose an approximate phrase mapping algorithm and incorporate the mapping score into the correlation measure . The correlations are further incorporated into a Maximum Entropy-based ranking model which estimates path weights from training . Experimental results show that our method significantly outperforms state-ofthe-art syntactic relation-based methods by up to 20 % in MRR . Answer Extraction is one of basic modules in open domain Question Answering ( QA ) . It is to further process relevant sentences extracted with Passage / Sentence Retrieval and pinpoint exact answers using more linguistic-motivated analysis . Since QA turns to find exact answers rather than text snippets in recent years , answer extraction becomes more and more crucial . Typically , answer extraction works in the following steps : • Recognize expected answer type of a question . • Annotate relevant sentences with various types of named entities . • Regard the phrases annotated with the expected answer type as candidate answers . • Rank candidate answers . In the above work flow , answer extraction heavily relies on named entity recognition ( NER ) . On one hand , NER reduces the number of candidate answers and eases answer ranking . On the other hand , the errors from NER directly degrade answer extraction performance . To our knowledge , most top ranked QA systems in TREC are supported by effective NER modules which may identify and classify more than 20 types of named entities ( NE ) , such as abbreviation , music , movie , etc . However , developing such named entity recognizer is not trivial . Up to now , we have n't found any paper relevant to QA-specific NER development . So , it is hard to follow their work . In this paper , we just use a general MUC-based NER , which makes our results reproducible . A general MUC-based NER ca n't annotate a large number of NE classes . In this case , all noun phrases in sentences are regarded as candidate answers , which makes candidate answer sets much larger than those filtered by a well developed NER . The larger candidate answer sets result in the more difficult answer extraction . Previous methods working on surface word level , such as density-based ranking and pattern matching , may not perform well . Deeper linguistic analysis has to be conducted . This paper proposes a statistical method which exploring correlation of dependency relation paths to rank candidate answers . It is motivated by the observation that relations between proper answers and question phrases in candidate sentences are always similar to the corresponding relations in question . For example , the question \" What did Alfred Nobel invent ? \" and the candidate sentence \" ... in the will of Swedish industrialist Alfred Nobel , who invented dynamite . \" For each question , firstly , dependency relation paths are defined and extracted from the question and each of its candidate sentences . Secondly , the paths from the question and the candidate sentence are paired according to question phrase mapping score . Thirdly , correlation between two paths of each pair is calculated by employing Dynamic Time Warping algorithm . The input of the calculation is correlations between dependency relations , which are estimated from a set of training path pairs . Lastly , a Maximum Entropy-based ranking model is proposed to incorporate the path correlations and rank candidate answers . Furthermore , sentence supportive measure are presented according to correlations of relation paths among question phrases . It is applied to re-rank the candidate answers extracted from the different candidate sentences . Considering phrases may provide more accurate information than individual words , we extract dependency relations on phrase level instead of word level . The experiment on TREC questions shows that our method significantly outperforms a densitybased method by 50 % in MRR and three stateof-the-art syntactic-based methods by up to 20 % in MRR . Furthermore , we classify questions by judging whether NER is used . We investigate how these methods perform on the two question sets . The results indicate that our method achieves better performance than the other syntactic-based methods on both question sets . Especially for more difficult questions , for which NER may not help , our method improves MRR by up to 31 % . The paper is organized as follows . Section 2 discusses related work and clarifies what is new in this paper . Section 3 presents relation path correlation in detail . Section 4 and 5 discuss how to incorporate the correlations for answer ranking and re-ranking . Section 6 reports experiment and results . In this paper , we propose a relation path correlation-based method to rank candidate answers in answer extraction . We extract and pair relation paths from questions and candidate sentences . Next , we measure the relation path correlation in each pair based on approximate phrase mapping score and relation sequence alignment , which is calculated by DTW algorithm . Lastly , a ME-based ranking model is proposed to incorporate the path correlations and rank candidate answers . The experiment on TREC questions shows that our method significantly outperforms a density-based method by 50 % in MRR and three state-of-the-art syntactic-based methods by up to 20 % in MRR . Furthermore , the method is especially effective for difficult questions , for which NER may not help . Therefore , it may be used to further enhance state-of-the-art QA systems even if they have a good NER . In the future , we are to further evaluate the method based on the overall performance of a QA system and adapt it to sentence retrieval task .\n",
      "\n",
      "Generated Summary:\n",
      " NER significantly outperforms a densitybased method by 50 % in MRR and three stateof-the-art syntactic-based methods by up to 20 % in MRR. the results indicate that our method achieves better performance than the other syntactic-based methods.\n"
     ]
    }
   ],
   "source": [
    "# Function to generate summary\n",
    "def generate_summary(text):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", max_length=1024, truncation=True)\n",
    "    summary_ids = model.generate(inputs.input_ids, max_length=150, min_length=40, length_penalty=2.0, num_beams=4, early_stopping=True)\n",
    "    return tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "\n",
    "# Example usage\n",
    "sample_paper = dataset[0]['document']\n",
    "print(\"Original Paper:\\n\", sample_paper)\n",
    "print(\"\\nGenerated Summary:\\n\", generate_summary(sample_paper))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation using ROGUE Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n"
     ]
    }
   ],
   "source": [
    "num_samples = min(100, len(dataset))\n",
    "print (num_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROUGE-1: 0.2325\n",
      "ROUGE-2: 0.0278\n",
      "ROUGE-L: 0.1690\n"
     ]
    }
   ],
   "source": [
    "# Initialize the ROUGE scorer\n",
    "scorer = rouge_scorer.RougeScorer([\"rouge1\", \"rouge2\", \"rougeL\"], use_stemmer=True)\n",
    "\n",
    "# Function to generate summary\n",
    "def generate_summary(text):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", max_length=1024, truncation=True)\n",
    "    summary_ids = model.generate(inputs.input_ids, max_length=150, min_length=40, length_penalty=2.0, num_beams=4, early_stopping=True)\n",
    "    return tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "\n",
    "# Function to evaluate the model using ROUGE\n",
    "def evaluate_model(dataset, num_samples=90):\n",
    "    all_rouge_scores = {\n",
    "        \"rouge1\": [],\n",
    "        \"rouge2\": [],\n",
    "        \"rougeL\": []\n",
    "    }\n",
    "\n",
    "    num_samples = min(100, len(dataset)-1)\n",
    "\n",
    "    for i in range(num_samples):\n",
    "        document = dataset[i]['document']\n",
    "        reference_summary = dataset[i]['outcome']\n",
    "        \n",
    "        generated_summary = generate_summary(document)\n",
    "        \n",
    "        scores = scorer.score(reference_summary, generated_summary)\n",
    "        \n",
    "        all_rouge_scores[\"rouge1\"].append(scores[\"rouge1\"].fmeasure)\n",
    "        all_rouge_scores[\"rouge2\"].append(scores[\"rouge2\"].fmeasure)\n",
    "        all_rouge_scores[\"rougeL\"].append(scores[\"rougeL\"].fmeasure)\n",
    "    \n",
    "    avg_rouge1 = sum(all_rouge_scores[\"rouge1\"]) / len(all_rouge_scores[\"rouge1\"])\n",
    "    avg_rouge2 = sum(all_rouge_scores[\"rouge2\"]) / len(all_rouge_scores[\"rouge2\"])\n",
    "    avg_rougeL = sum(all_rouge_scores[\"rougeL\"]) / len(all_rouge_scores[\"rougeL\"])\n",
    "\n",
    "    return {\n",
    "        \"rouge1\": avg_rouge1,\n",
    "        \"rouge2\": avg_rouge2,\n",
    "        \"rougeL\": avg_rougeL\n",
    "    }\n",
    "\n",
    "# Evaluate the model on 100 samples\n",
    "evaluation_results = evaluate_model(tokenized_test_data, num_samples=100)\n",
    "\n",
    "# Print the evaluation results\n",
    "print(f\"ROUGE-1: {evaluation_results['rouge1']:.4f}\")\n",
    "print(f\"ROUGE-2: {evaluation_results['rouge2']:.4f}\")\n",
    "print(f\"ROUGE-L: {evaluation_results['rougeL']:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion and Insights"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
